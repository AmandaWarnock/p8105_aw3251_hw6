---
title: "Homework 6"
author: Amanda Warnock
output: github_document
---

This is my solution to HW6.

```{r}
library(tidyverse)
library(modelr)
library(mgcv)
```


## Problem 1

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>% 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest" ~0,
      disposition == "Closed by arrest" ~ 1)
    ) %>% 
  filter(
    city_state !="Tulsa, AL",
    victim_race %in% c("White", "Black")) %>% 
  select(city_state, resolution, victim_age, victim_race, victim_sex)
  
```

```{r}
baltimore_df = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD")

glm(resolution ~ victim_age + victim_race + victim_sex,
    data = baltimore_df,
    family = binomial()) %>% 
  broom::tidy() %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(term, OR, starts_with("CI")) %>% 
  knitr::kable(digits=3)
```
^remember that you need to exponentiate the estimates to be able to compare them to 1

try across cities
```{r}
model_results_df = 
homicide_df %>% 
  nest(data = -city_state) %>% 
  mutate(
    models = map(.x = data, ~glm(resolution ~ victim_age + victim_race + victim_sex, data = .x,
    family = binomial())),
    results = map(models, broom::tidy)
  ) %>% 
  select(city_state, results) %>% 
  unnest(results) %>% 
  mutate(
    OR = exp(estimate),
    CI_lower = exp(estimate - 1.96 * std.error),
    CI_upper = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, OR, starts_with("CI"))
```

```{r}
model_results_df %>% 
  filter(term == "victim_sexMale") %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>% 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper)) +
theme(axis.text.x = element_text(angle = 90))
```

## Problem 2

Loading data.

```{r}
bw_df =
  read.csv("data/birthweight.csv") %>% 
  janitor::clean_names()
```

Cleaning data.


```{r}
bw_df = 
bw_df %>% 
  mutate(
    babysex = as.character(babysex),
    babysex = case_when(
      babysex == 1 ~ "Male",
      babysex == 2 ~ "Female"),
    frace = as.character(frace),
    frace = case_when(
      frace == 1 ~ "White",
      frace == 2 ~ "Black",
      frace == 3 ~ "Asian",
      frace == 4 ~ "Puerto Rican",
      frace == 8 ~ "Other",
      frace == 9 ~ "Unknown"),
    gaweeks = as.double(gaweeks),
    malform = as.character(malform),
    malform = case_when(
      malform == 0 ~ "absent",
      malform == 1 ~ "present"),
    mrace = as.character(mrace),
    mrace = case_when(
      mrace == 1 ~ "White",
      mrace == 2 ~ "Black",
      mrace == 3 ~ "Asian",
      mrace == 4 ~ "Puerto Rican",
      mrace == 8 ~ "Other"),
    ppbmi = as.double(ppbmi),
    smoken = as.double(smoken)
    )
```

Income, smoking, and the age of the mother have all been found to be associated with birthweight. Generally, it can be hypothesized that have a lower income, more cigarettes per day, and an older age may result in lower birthweight. I'm curious to look into these factors to see how much of an impact they appear to have on birthweight. 

Here, I set up the linear regression and clean the output.

```{r}
fit = lm(bwt ~ fincome + smoken + momage, data = bw_df)
summary(fit)

fit_table = 
fit %>% 
  broom::tidy() %>% 
  select(term, estimate, p.value) %>% 
  mutate(
    term = str_replace(term, "^fincome", "Family monthly income in hundreds"),
    term = str_replace(term, "^smoken", "Av. cigarettes per day during pregnancy"),
    term = str_replace (term, "^momage", "Age of mom at delivery")) %>% 
  knitr::kable()

fit_table
```

Exploratory analysis.

```{r}
fincome_resid = 
bw_df %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x =  fincome, y = resid)) + geom_violin()

fincome_pred = 
  bw_df %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = fincome, y  = pred)) + geom_violin()
```

```{r}
smoken_resid = 
bw_df %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = smoken, y = resid)) + geom_violin()

smoken_pred = 
  bw_df %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = smoken, y = pred)) + geom_violin()
```

```{r}
momage_resid = 
  bw_df %>% 
  modelr::add_residuals(fit) %>% 
  ggplot(aes(x = momage, y = resid)) + geom_violin()

momage_pred = 
  bw_df %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = momage, y = pred)) + geom_violin()
```

Plot of residuals. 

```{r}
resid_pred = 
bw_df %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit)  %>% 
  ggplot(aes(x = resid, y = pred)) + geom_point()

resid_pred
```

Residuals tend to cluster around 0 and predictions tend to cluster around 3100. 

Compare to other models. 

```{r}
fit2 = lm(bwt ~ blength + gaweeks, data = bw_df)
summary(fit2)

fit2_resid_pred = 
  bw_df %>% 
  modelr::add_residuals(fit2) %>% 
  modelr::add_predictions(fit2) %>% 
  ggplot(aes(x = resid, y = pred)) + geom_point()

fit2_resid_pred
```

While these resdiuals also cluster around 0 and the predictions cluster around 3000, there are some major outliers.  

```{r}
fit3 = lm(bwt ~ bhead + blength + babysex, data = bw_df)
summary(fit3)

fit3_resid_pred = 
  bw_df  %>% 
  modelr::add_residuals(fit3) %>% 
  modelr::add_predictions(fit3) %>% 
  ggplot(aes(x = resid, y = pred)) + geom_point()

fit3_resid_pred
```

Overall clustering is similar, but there are also multiple major outliers. 

Comparing via cross-validation.

```{r}
cv_df = 
crossv_mc(bw_df, 100) %>% 
  mutate(
    train  = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df =  
  cv_df %>% 
  mutate(
    fit1  = map(train, ~lm(bwt ~ fincome + smoken + momage, data = .x)),
    fit2 =  map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    fit3 = map(train, ~lm(bwt ~ bhead + blength + babysex, data = .x))) %>% 
  mutate(
    rmse1 = map2_dbl(fit1, test, ~rmse(model  = .x,  data = .y)), 
    rmse2 =  map2_dbl(fit2, test, ~rmse(model = .x, data = .y)),
    rmse3 = map2_dbl(fit3, test, ~rmse(model =  .x, data = .y))
  )

cv_df
```

```{r}
cv_comparison = 
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to  = "model",
    values_to  = "rsme",
    names_prefix =  "rsme_") %>% 
  ggplot(aes(x = model, y  = rsme)) + geom_violin()

cv_comparison
```

The RSME for my model, RMSE1, are far higher than for the other two models. The model using head circumference, length, and sex has the lowest RSME. 

## Problem 3

Loading data. 

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())

set.seed(1)
```

Fitting the linear model.

```{r}
fit = lm(tmax ~tmin, data = weather_df)
summary(fit)

fit_output = 
fit %>% 
  broom::tidy() %>% 
  knitr::kable()

fit_output 
```

Bootstrap function and plot. 

```{r}
boot_sample = function(df) {
  sample_frac(df, replace = T) %>% 
    arrange(tmin)
}

boot_sample(weather_df) %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_point() +
  stat_smooth(method = "lm")
```

Taking 5000 bootstrap samples. 

```{r}
bootstraps = 
  data_frame(
    strap_number = 1:5000,
    strap_sample = rerun(5000, boot_sample(weather_df))
  )

bootstraps
```

Cleaning the output to focus on the selected quantities and creating the log equation.

```{r}
bootstraps_output = 
  bootstraps %>% 
  mutate(
    model = map(strap_sample, ~lm(tmax ~ tmin, data = .x)),
    broomtidy = map(model, broom::tidy),
    broomglance = map(model, broom::glance)
  ) %>% 
  unnest(broomtidy, broomglance) %>% 
  select(term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = term,
    values_from = estimate
  ) %>% 
  rename(
    B0 = '(Intercept)',
    B1 = tmin,
    rsq = r.squared
  ) %>% 
  mutate(log_eq = log(B0*B1)) %>% 
  select(rsq, log_eq)

bootstraps_output
```

Creating a plot of the log equation

```{r}
log_curve = 
bootstraps_output %>% 
  ggplot(aes(x = log_eq)) +
  geom_histogram() +
  theme_bw() +
  labs(
    y = "Frequency",
    x = "log(B0*B1) Estimate"
  )

log_curve
```

The 95% CI for log(B0*B1) is [`r round(quantile(pull(bootstraps_output, log_eq), probs = c(0.025, 0.975)), digits = 2)`].

Creating a plot of the R2. 

```{r}
rsq_curve  = 
bootstraps_output %>% 
  ggplot(aes(x = rsq)) +
  geom_histogram() +
  labs(
    y = "Frequency",
    x = "R2 Estimate"
  )

rsq_curve
```

The 95% CI for r2 is [`r round(quantile(pull(bootstraps_output, rsq), probs = c(0.025, 0.975)), digits = 2)`].


